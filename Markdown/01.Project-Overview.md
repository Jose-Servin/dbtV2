# Getting started with dbt

## Project Overview

Role: Data Engineer at Airbnb

General Tasks:

* Load, clean and expose data
* Write tests, automation and documentation

Data source:

* Inside Airbnb Berlin data

Tech Stack

* dbt, Snowflake and Preset (BI)

## Step 1: dbt + Snowflake Setup

After signing up for a trial account, we have to perform the following steps in Snowflake.

1. Setup dbt user

2. Create Airbnb database

See [notes](https://github.com/nordquant/complete-dbt-bootcamp-zero-to-hero/blob/main/_course_resources/course-resources.md)

## Step 2: dbt configuration

Next, we go into our dbt project repo and run through these steps:

1. We run `mkdir ~.dbt`.

2. In our project folder `dbtReview` we run `dbt init dbtlearnV2` and answer setup questions.

3. We `cd dbtlearnV2` and run `dbt debug` to verify if the Snowflake connection was successful.

## Understanding our dbt Folder Structure

From our `dbtReview` folder, if we run `tree` we see

 ```text
.
├── dbtlearnV2
│   ├── README.md
│   ├── analyses
│   ├── dbt_project.yml
│   ├── logs
│   │   └── dbt.log
│   ├── macros
│   ├── models
│   │   └── example
│   │       ├── my_first_dbt_model.sql
│   │       ├── my_second_dbt_model.sql
│   │       └── schema.yml
│   ├── seeds
│   ├── snapshots
│   └── tests
└── logs
    └── dbt.log
 ```

### dbt_project.yml

dbt project level configurations. Here we will see project folder paths defined as well as how `models` get materialized.

`name:` is the name we provided when running `dbt init {project-name}`.

Here, we deleted the settings configurations for `models` and the 2 example `models` created by default.

### dbt Power User

After installing the VSCode extension, we created a `.vscode/` directory to hold our workspace `settings.json` file.

```json
{"files.associations": {
    "*.sql" : "jinja-sql"
}}
```

## Step 3: Building our Data Flow

### Input Data Models

1. Listings
2. Hosts
3. Reviews
4. Full Moon Dates

Note: Full Moon Dates will help us see if a correlation exists between full moons and Airbnb review sentiments. "Do full moons negatively or positively affect Airbnb reviews?"

![Data Flow](../images/data-flow.png)

![Data DAG](../images/data-DAG.png)

## Source Models

The first Models we will create in our dbt project is our `src_{model-name}.sql` "source" models.

* src_hosts
* src_listings
* src_reviews

![Source Models](../images/source-models.png)

After defining our `src_listings` in Snowflake, we take our query and create a `models/src/src_listings.sql` file for dbt to execute with `dbt run`. Reminder to make sure we are in our `dbtlearnV2` folder.

![dbt run](../images/dbt-run.png)

## Materialization Overview

![Materialization Notes](../images/materialization-notes.png)

## Core Layer Models

After building our source models, we move on to the "Core Layer."

![Core Layer Models](../images/core-layer-models.png)

When building the core layer models, it's best practice to be explicit with our model's materialization. To achieve this, we will add a global configuration to our `dbt_project.yml` file.

```yml
# dtb_project.yml
models:
  dbtlearnV2:
    +materialized: view
```

After applying this configuration, we then materialize our `dim` models as tables because they represent "ready to use" data and will be used often for downstream model creation. If we leave our `dim` models are views, we will be executing the underlying sql statement each time we reference the dim model.

```yml
models:
  dbtlearnV2:
    +materialized: view
    dim:
      +materialized: table
```

![View Materialization](../images/view-materialization.png)

After running `dbt run` we see

![Table Materialization](../images/table-materialization.png)

Our `dim_hosts_cleaned.sql` model has an materialization as view config in the .sql file so it does not get materialized as a table. This shows us we can control each model's materialization independent from our global configurations.

### Incremental Materialization

For our `fct_reviews` model, we will be using an incremental materialization.

```sql
{{
    config(
        materialized = 'incremental',
        on_schema_change='fail'
    )
}}
WITH src_reviews AS (
    SELECT * FROM {{ ref('src_reviews') }}
)

SELECT * FROM src_reviews
WHERE review_text is not null

{% if is_incremental() %}
    AND review_date > (select max(review_date) from {{ this }})
{% endif %}
```

The incremental part of this model comes from the `.sql` configuration we define at the top of our sql file. Note: `this` refers to our `fct_reviews` model.

With this defined `incremental` materialization, we next define what logic dbt should use to increment. "What defines a new record?"

For `fct_reviews`, we are saying a new record(s) are any from `src_reviews` that have a review date greater than the max review date currently present in our `fct_reviews` table. Now, we have to go one step further and look at `src_reviews` which is a view built from `raw_reviews`. This is why, when we test the incremental loading, we will add a record to `raw_reviews` and not directly to `src_reviews`. If we were to add this new record to `src_reviews` then it would contain a record not present in it's source which is the wrong way to go about this incremental load building.

This is why on the first `dbt run` all records from `src_reviews` are added to the `fct_reviews` model because there is no max date present, the model does not "exist."

To test this incremental loading, we can take a look at listing `3176` from our `fct_reviews` model. We then add a record to our `raw_reviews` model and perform a `dbt run --full-refresh` to "pick up the changes." First, `src_reviews` will get re-built with this new record and then `fct_reviews` will get this record incrementally added.

Row added:

```sql
INSERT INTO "AIRBNB"."RAW"."RAW_REVIEWS"
VALUES (3176, CURRENT_TIMESTAMP(), 'Servin', 'excellent stay!', 'positive');
```

Verification:

```sql
SELECT * FROM AIRBNB.RAW.RAW_REVIEWS
WHERE REVIEWER_NAME = 'Servin';
```

`dbt run --full-refresh`

```terminal
14:15:49  Finished running 4 view models, 1 table model, 1 incremental model in 0 hours 0 minutes and 12.72 seconds (12.72s).
14:15:49
14:15:49  Completed successfully
14:15:49
14:15:49  Done. PASS=6 WARN=0 ERROR=0 SKIP=0 TOTAL=6
```

Snowflake Verification:

After running this Snowflake verification script, we should see our 'Servin' review in our `fct_reviews` model.

```sql
SELECT * FROM "AIRBNB"."DEV"."FCT_REVIEWS" WHERE listing_id=3176;
```

### Ephemeral Materialization

They strategy here is to materialize `dim_listings_with_hosts` and `fct_reviews` as tables because they represent our "final" tables that will be used by downstream users/BI.

`dim_listings_cleaned` and `dim_hosts_cleaned` remain as views and our `src_` models are converted to ephemeral models; this way, they remain as CTEs and are only "built" when upstream `dim_` models materialize.

So, project wise, we change our dbt configs to be:

```yml
models:
  dbtlearnV2:
    +materialized: view
    dim:
      +materialized: view
    src:
      +materialized: ephemeral
```

Our `dim_listings_w_hosts` get's a in-line configuration to overwrite the global config we just set; this is because it's in our `dim` folder which we just set to materialize as `view`.

```sql
-- dim_listings_w_hosts.sql
{{
    config(
        materialized = 'table'
    )
}}

```

Lastly, since these `src_` views already exist in Snowflake, dbt will not drop them so we have to manually drop all `src_` views. Once we do that, we can then run `dbt run` and see how only `4` models are being materialized and not `7` like before. Again, this is because our `src_` models are not ephemeral and remain CTEs.

```terminal
02:33:22  1 of 4 START sql view model DEV.dim_hosts_cleaned .............................. [RUN]
02:33:22  1 of 4 OK created sql view model DEV.dim_hosts_cleaned ......................... [SUCCESS 1 in 0.64s]
02:33:22  2 of 4 START sql view model DEV.dim_listings_cleaned ........................... [RUN]
02:33:23  2 of 4 OK created sql view model DEV.dim_listings_cleaned ...................... [SUCCESS 1 in 0.69s]
02:33:23  3 of 4 START sql incremental model DEV.fct_reviews ............................. [RUN]
02:33:28  3 of 4 OK created sql incremental model DEV.fct_reviews ........................ [SUCCESS 1 in 4.73s]
02:33:28  4 of 4 START sql table model DEV.dim_listings_w_hosts .......................... [RUN]
02:33:29  4 of 4 OK created sql table model DEV.dim_listings_w_hosts ..................... [SUCCESS 1 in 1.30s]
02:33:29
```

![Table Materialization](../images/model-materialization.png)

### Locating Ephemeral Models (Target Folder)

So where are these ephemeral models located? Well, in our `dbt_project.yml` file we see a `target-path` configuration defined which states, "directory which store compiles SQL files" If this does not exists, add the following configuration:

```yml
target-path: "target" # directory which store compiles SQL files
clean-targets: # directories to be removed by `dbt clean`
  - "target"
  - "dbt_packages"
```

Here, we are interested in the `run` directory located in our `target` folder.

```terminal
.
{other-folders-here}
├── run
│   └── dbtlearnV2
│       └── models
│           ├── dim
│           │   ├── dim_hosts_cleaned.sql
│           │   ├── dim_listings_cleaned.sql
│           │   └── dim_listings_w_hosts.sql
│           ├── fct
│           │   └── fct_reviews.sql
│           └── src
│               ├── src_hosts.sql
│               ├── src_listings.sql
│               └── src_reviews.sql
├── run_results.json
└── semantic_manifest.json
```

This will show us the final compiles sql for each model we have defined in our project. For example, if we take a look at `dim_listing_cleaned.sql`

`../dbtlearnV2/target/run/dbtlearnV2/models/dim/dim_listings_cleaned.sql`

```sql
-- compiled sql for dim_listings_cleaned model
  create or replace   view AIRBNB.DEV.dim_listings_cleaned

   as (
    WITH  __dbt__cte__src_listings as (
WITH raw_listings AS (
    SELECT
        *
    FROM
        AIRBNB.RAW.RAW_LISTINGS
)
SELECT
    id AS listing_id,
    name AS listing_name,
    listing_url,
    room_type,
    minimum_nights,
    host_id,
    price AS price_str,
    created_at,
    updated_at
FROM
    raw_listings
), src_listings AS (
  SELECT
    *
  FROM
    __dbt__cte__src_listings
)
SELECT
  listing_id,
  listing_name,
  room_type,
  CASE
    WHEN minimum_nights = 0 THEN 1
    ELSE minimum_nights
  END AS minimum_nights,
  host_id,
  REPLACE(
    price_str,
    '$'
  ) :: NUMBER(
    10,
    2
  ) AS price,
  created_at,
  updated_at
FROM
  src_listings
  );
```

And here is where we can see our `ephemeral` CTE `src_` models being referenced by dbt with an internal identifier, `__dbt__cte__src_listings`. Also, this `target` folder is where we will go first to debug any failed dbt run commands since it will show us the full compiled sql being executed to create our models.
